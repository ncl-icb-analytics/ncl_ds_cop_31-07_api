{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 2 - Stretch Objectives\n",
    "\n",
    "## Objectives\n",
    "* Put the API request code into a function that can be called repeatedly\n",
    "* Iterate to make multiple API requests\n",
    "* Add some basic error handling\n",
    "* Output the results to the sandpit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "#New imports\n",
    "import ncl_sqlsnippets as snips\n",
    "import time\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the API Request Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function version of the API request \n",
    "##This function can be used to repeatedly make requests from the API\n",
    "##The function takes a hash_id as an input to specify which site to request data for\n",
    "##You may want to also convert the response into a tabular form in the function\n",
    "def request_smart(hash_id):  \n",
    "    ####ENTER CODE HERE###############\n",
    "    \n",
    "    ##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your function with the code below\n",
    "\n",
    "Does the function still work when you change which hash_id you give it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_response = request_smart(hash_id=\"S0189179\")\n",
    "df_response.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate to make multiple requests\n",
    "\n",
    "Practically we would want our code to fetch data for all of our sites\n",
    "We can use a simple loop to do this across multiple requests\n",
    "\n",
    "This code may take a minute to handle all the requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full list of sites\n",
    "site_list = [\"S0189179\",\"S89CFECF\",\"S818235B\",\"SF06F50A\",\"S48446E1\",\"S12C3F7C\"]\n",
    "\n",
    "#Define an empty data frame to store the combined results in\n",
    "df_combined = pd.DataFrame()\n",
    "\n",
    "\n",
    "for site in site_list:\n",
    "    new_response = request_smart(hash_id=site)\n",
    "    time.sleep(5)\n",
    "    print(f\"Processed site {site}\")\n",
    "    df_combined = pd.concat([df_combined, new_response])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many rows are in the combined table\n",
    "df_combined.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Handling\n",
    "\n",
    "Try running the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "while(True):\n",
    "    idx += 1\n",
    "    print(idx)\n",
    "    request_smart(hash_id=\"S0189179\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get an error after a number of requests?\n",
    "\n",
    "Do you understand what caused this error?\n",
    "\n",
    "A common secruity measure for APIs is to block sources making an \"excessive\" number of requests. This is because a common method for cyber attacks is a DDOS attack which works by making endless requests to the website in quick sucession in an attempt to overwhelm the web server and cause the site to crash.\n",
    "\n",
    "Unfortunately, the SMART API has a fairly low threshold of what it considers to be \"excessive\" and will block all requests for 1 minute when breached.\n",
    "\n",
    "To get around this we can add an intentional delay to our code to space out our requests. In the for loop before this was done with the line `time.sleep(5)` which adds a 5 second delay before making the next request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing we may want to consider is a backup mechanism if we accidentally cross this threshold. Since we know the timeout is 1 minute we can ask the code to wait for a minute when we run into this issue. You can see this implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full list of sites\n",
    "site_list = [\"S0189179\",\"S89CFECF\",\"S818235B\",\"SF06F50A\",\"S48446E1\",\"S12C3F7C\"]\n",
    "\n",
    "#Define an empty data frame to store the combined results in\n",
    "df_combined = pd.DataFrame()\n",
    "\n",
    "#Specify the cool off period\n",
    "cooloff = 60\n",
    "\n",
    "for site in site_list:\n",
    "    #First attempt at making the request\n",
    "    try:\n",
    "        new_response = request_smart(hash_id=site)\n",
    "        print(f\"Processed site {site}\")\n",
    "        df_combined = pd.concat([df_combined, new_response])\n",
    "\n",
    "    except:\n",
    "        print(f\"API Overloaded, waiting before reattempting...\")\n",
    "        time.sleep(cooloff)\n",
    "\n",
    "        #Second attempt at making the request\n",
    "        try:\n",
    "            new_response = request_smart(hash_id=site)\n",
    "            print(f\"Processed site {site}\")\n",
    "            df_combined = pd.concat([df_combined, new_response])\n",
    "        except:\n",
    "            raise Exception(\n",
    "                \"Failed twice for the same request so cancelling execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the result to the Sandpit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up destination parameters\n",
    "\n",
    "##MAKE SURE YOU DELETE WHAT EVER TEST TABLE YOU CREATE AFTER THE SESSION##\n",
    "\n",
    "####ENTER CODE HERE###############\n",
    "sql_address = 'PSFADHSSTP01.AD.ELC.NHS.UK,1460'\n",
    "sql_database = 'Data_Lab_NCL_Dev'\n",
    "sql_schema = 'JakeK'\n",
    "sql_table = 'ds_cop_smart_test'\n",
    "##################################\n",
    "\n",
    "#Connect to the sandpit\n",
    "\n",
    "sql_driver = \"SQL+Server\"\n",
    "sql_servertype = \"mssql\"\n",
    "\n",
    "connection_string = (f\"{sql_servertype}://{sql_address}/{sql_database}\"\n",
    "                     + f\"?trusted_connection=yes&driver={sql_driver}\")\n",
    "\n",
    "print(\"This is the connection string:\", connection_string)\n",
    "\n",
    "engine = create_engine(connection_string, use_setinputsizes=False)\n",
    "\n",
    "df_combined.to_sql(sql_table, engine, \n",
    "                   schema=sql_schema, if_exists=\"replace\", \n",
    "                   index=False, chunksize=75, method=\"multi\")\n",
    "\n",
    "#This can also be done with the ncl_sqlsnippets package with the following code:\n",
    "\n",
    "#engine = snips.connect(sql_address, sql_database)\n",
    "#snips.upload_to_sql(df_combined, engine, sql_table, sql_schema, \n",
    "#                    replace=True, chunks=75)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
